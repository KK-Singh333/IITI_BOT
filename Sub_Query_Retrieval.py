# -*- coding: utf-8 -*-
"""Draft 3_IITI-BOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WtsaMy8BiSmkCTAtqOkuDuN6-O2_pWIz
"""


!pip install pathway pathway[xpack-llm] litellm

!pip install pathway

import pathway as pw
from pathway.xpacks.llm import llms
# from pathway.udfs import ExponentialBackoffRetryStrategy

import os
# from getpass import getpass
os.environ['GROQ_API_KEY'] = os.getenv("GROQ_API_KEY") or "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"

# Setting model
from pathway.udfs import ExponentialBackoffRetryStrategy
llm = llms.LiteLLMChat(model="groq/meta-llama/llama-4-scout-17b-16e-instruct", retry_strategy=ExponentialBackoffRetryStrategy(max_retries=2), temperature = 0.0)

def initiate_chat(user_id:str, query:str):
  system_prompt = """
  You are an AI language model assistant.
  Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database.
  By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.
  Provide these alternative questions separated by newlines.
  Output only the generated queries not including any other text.
  """
  messages = pw.debug.table_from_rows(
      schema = pw.schema_from_types(user_id = str, questions=list[dict]),
      rows=[
          (
              f"{user_id}",
              [
                  {"role": "system", "content": system_prompt},
                  {"role": "user", "content": f"{query}"},
              ],
          )
      ],
  )


responses = messages.select(user_id = pw.this.user_id, result=llm(pw.this.questions))

pw.debug.compute_and_print(messages)
pw.debug.compute_and_print(responses)

@pw.udf
def split_lines(text: str) -> list[str]:
    return text.splitlines()

split_table = responses.select(
    user_id=pw.this.user_id,
    questions = split_lines(pw.this.result)  # This gives a List[str]
)
response = split_table.flatten(pw.this.questions)
pw.debug.compute_and_print(split_table)
pw.debug.compute_and_print(response)



# DocStore
import pathway as pw
from pathway.xpacks.llm import embedders

embedder = embedders.SentenceTransformerEmbedder(model="all-MiniLM-L6-v2")

# Create a table with text to embed
t = pw.debug.table_from_rows(
    schema = pw.schema_from_types(doc_id = int, chunks = str),
    rows=[
        (
            1,
            "Facilities1. 24 hrs high-speed internet facility.2. Indoor Sports facilities."
        ),
        (
            2,
            """facilities.3. Badminton Court.4. Common Room Facility.5. Newspaper and Magazine in the common area.6. Laundry Facility.7. Equipped with basic furniture like cot, study table, and an almirah for storage.8. CCTV Surveillance and 24 Hrs. Security Guard in Each Hostel."""
        )
    ],
)

# Extract the embedded text
vector_store = t.with_columns(embedding=embedder(pw.this.chunks))
# pw.debug.compute_and_print(t)
vector_store.typehints()



class InputSchema(pw.Schema):
    row_id: str
    chunk: str
    embedding: list[list[float]]
    url: str

vector_store = pw.io.csv.read(
  'embedding_sample.csv',
  schema=InputSchema,
  mode="static"
)

# Retrieval

# Queries ko embedd kar dete hai
response += response.select(embedding=embedder(pw.this.questions))

from pathway.stdlib.ml.index import KNNIndex

doc_index = KNNIndex(
    vector_store.embedding,
    vector_store,
    n_dimensions= embedder.get_embedding_dimension(),  # dimension for all-MiniLM-L6-v2
    distance_type = "cosine"
    # n_and_d=2
)

results = doc_index.get_nearest_items(
    response.embedding,
    k=1  # top 5 most similar documents
).select(doc_id = pw.this.doc_id, chunks = pw.this.chunks)

pw.debug.compute_and_print(results)
