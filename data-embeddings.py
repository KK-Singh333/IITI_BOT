# -*- coding: utf-8 -*-
"""IITI_Bot_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13wA-RPxc_mGYhYu96gp_fs3lRrygUBMU
"""

# Install core dependencies
!pip install sentence-transformers

# Install Pathway nightly with xpacks from official index
!pip install pathway

from google.colab import files

# Upload the CSV file
uploaded = files.upload()
# Your file (e.g. iiti_data_merged_final.csv) will be available in /content/

import pathway as pw
from pathway.xpacks.llm.splitters import RecursiveSplitter

# ✅ Step 1: Define schema for your CSV
class IITIWebSchema(pw.Schema):
    id: int
    url: str
    title: str
    body_text: str
    source_domain: str
    metadata: str

# ✅ Step 2: Load your CSV
init_table = pw.io.csv.read(
    "iiti_data_merged_final.csv",
    schema=IITIWebSchema,
    mode="streaming",
    autocommit_duration_ms=600000
)

# Optionally check for non-empty body_text or metadata instead
final_table = init_table.filter(pw.this.body_text != "")

# ✅ Step 3: Setup RecursiveSplitter
splitter = RecursiveSplitter(
    chunk_size=500,
    chunk_overlap=150,
    separators=["\n#", "\n##", "\n\n", "\n","."],
    model_name="gpt-4o-mini",
)

# ✅ Step 4: Apply splitter (use row_id instead of id)
chunked = final_table.select(
    row_id=pw.this.id,
    url=pw.this.url,
    title=pw.this.title,
    metadata=pw.this.metadata,
    chunks=splitter(pw.this.body_text)
)

# ✅ Step 5: Flatten the chunks
flattened = chunked.flatten(pw.this.chunks)

# ✅ Step 6: Save to CSV for inspection
pw.io.csv.write(
    table=flattened,
    filename="output_chunks.csv"
)

# ✅ Step 7: Run the pipeline
pw.run()

from sentence_transformers import SentenceTransformer
import numpy as np
from pathway.xpacks.llm.splitters import RecursiveSplitter



# Load your model
model = SentenceTransformer("all-MiniLM-L6-v2")

# UDF to generate embeddings
@pw.udf
def batch_embedding(texts: list[str]) -> list[list[float]]:
    return model.encode(texts).tolist()

# Add embeddings to each chunk
embedded = flattened.select(
    row_id=pw.this.row_id,
    chunk=pw.this.chunks,
    embedding=batch_embedding(pw.this.chunks),  # This batches internally
    url=pw.this.url,
    title=pw.this.title,
    metadata=pw.this.metadata
)

pw.io.csv.write(
    table=embedded,
    filename="final_embedded_chunks.csv"
)

pw.run()
